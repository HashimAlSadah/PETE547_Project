{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt for solving navier-stoke equation using PINNS\n",
    "\n",
    "let us take the following PDE \n",
    "$$\n",
    "   \n",
    "    \\frac{\\partial u}{\\partial t} \n",
    "    + u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y}\n",
    "    = \n",
    "    -\\frac{1}{\\rho} \\frac{\\partial p}{\\partial x} \n",
    "    + \\nu \\left( \n",
    "    \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\n",
    "    \\right) \\\\[10pt]\n",
    "\n",
    "  \n",
    "    \\frac{\\partial v}{\\partial t} \n",
    "    + u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y}\n",
    "    = \n",
    "    -\\frac{1}{\\rho} \\frac{\\partial p}{\\partial y} \n",
    "    + \\nu \\left( \n",
    "    \\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2}\n",
    "    \\right)\n",
    "    \\\\[10pt]\n",
    "   \n",
    "    \\left( \\frac{\\partial u}{\\partial x} \\right)^2 +\n",
    "    2 \\left( \\frac{\\partial u}{\\partial y} \\right)\n",
    "    \\left( \\frac{\\partial v}{\\partial x} \\right) +\n",
    "    \\left( \\frac{\\partial v}{\\partial y} \\right)^2\n",
    "    = -\\frac{1}{\\rho} \n",
    "    \\left(\n",
    "    \\frac{\\partial^2 p}{\\partial x^2} \n",
    "    + \\frac{\\partial^2 p}{\\partial y^2}\n",
    "    \\right)\n",
    "    \n",
    "$$\n",
    "\n",
    "\n",
    "The boundary conditions for the velocity are the following.\n",
    "$$\n",
    "u(t, x, l_y) = 1, \\quad u(t, x, 0) = 0, \\quad\n",
    "u(t, 0, y) = 0, \\quad u(t, l_x, y) = 0\n",
    "$$\n",
    "$$\n",
    "v(t, x, l_y) = 0, \\quad v(t, x, 0) = 0, \\quad\n",
    "v(t, 0, y) = 0, \\quad v(t, l_x, y) = 0\n",
    "$$\n",
    "\n",
    "consider $l_x = l_y = 1$\n",
    "\n",
    "The boundary condition for the pressure\n",
    "$$\n",
    "\\frac{\\partial p}{\\partial x}\\Big|_{x=0} = 0, \\quad \n",
    "\\frac{\\partial p}{\\partial x}\\Big|_{x=l_x} = 0, \\quad \n",
    "\\frac{\\partial p}{\\partial y}\\Big|_{y=0} = 0, \\quad \n",
    "p(t, x, l_y) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PINNs attempt\n",
    "\n",
    "We are going to train a neural network on the boundary conditions and the pde as constraints to find $u, v \\& p$ \n",
    "\n",
    "The neural networkd is going to take 3 inputs (time, x, y) and produces 3 output (u, v, p), thus we need to initialize the inputs with the right dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to have $t \\in [0,0.1]$ since we only want to see how system evolves in a small time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0, x_f, y_0, y_f, t_0, t_f = 0.0, 1.0, 0.0, 1.0, 0.0 , 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(x_num = 10, y_num=10, t_num=10):\n",
    "    x_axis = torch.linspace(x_0, x_f,  x_num)\n",
    "    y_axis = torch.linspace(y_0, y_f, y_num )\n",
    "    t_axis = torch.linspace(t_0, t_f, t_num )\n",
    "\n",
    "\n",
    "    data = torch.cartesian_prod(x_axis, y_axis, t_axis)\n",
    "\n",
    "    return data[:, 0].reshape(-1,1), data[:, 1].reshape(-1,1), data[:, 2].reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
    "        super().__init__()\n",
    "        activation = nn.Tanh\n",
    "        self.fcs = nn.Sequential(*[\n",
    "            nn.Linear(N_INPUT, N_HIDDEN),\n",
    "            activation()\n",
    "\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.fch =nn.Sequential(*[ nn.Sequential(*[\n",
    "            nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "            activation()\n",
    "\n",
    "        ]) for _ in range(N_LAYERS-1)])\n",
    "        self.fce = nn.Linear(N_HIDDEN,N_OUTPUT)\n",
    "        def weights_initialization(self):\n",
    "                \"\"\"\"\n",
    "                When we define all the modules such as the layers in '__init__()'\n",
    "                method above, these are all stored in 'self.modules()'.\n",
    "                We go through each module one by one. This is the entire network,\n",
    "                basically.\n",
    "                \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight,gain=1.0)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def forward(self, x, y, t):\n",
    "        inputs = torch.cat((x, y, t), 1)\n",
    "        inputs = self.fcs(inputs)\n",
    "        inputs = self.fch(inputs)\n",
    "        output = self.fce(inputs)\n",
    "        \n",
    "        # Split the output into u, v, and p\n",
    "        u, v, p = torch.split(output, 1, dim=1)\n",
    "        \n",
    "\n",
    "        return u, v, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_physics , y_physics, t_physics = generate_data()\n",
    "x_physics = x_physics.requires_grad_()\n",
    "y_physics = y_physics.requires_grad_()\n",
    "t_physics = t_physics.requires_grad_()\n",
    "\n",
    "examples_num = len(x_physics)\n",
    "#Boundary \n",
    "y_up = torch.tensor( [y_f]*examples_num).unsqueeze(-1)\n",
    "y_down = torch.tensor( [y_0]*examples_num).unsqueeze(-1).requires_grad_()\n",
    "x_right = torch.tensor( [x_f]*examples_num).unsqueeze(-1).requires_grad_()\n",
    "x_left = torch.tensor( [x_f]*examples_num).unsqueeze(-1).requires_grad_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_loss(x,y,t,net,rho=1, train = 10):\n",
    "    u, v, p = net(x,y,t) \n",
    "    u_x = torch.autograd.grad(u, x,torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u_x),create_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u, y,torch.ones_like(u), create_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y, y,torch.ones_like(u_y), create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    v_x = torch.autograd.grad(v, x,torch.ones_like(v), create_graph=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x, x, torch.ones_like(v_x),create_graph=True)[0]\n",
    "    v_y = torch.autograd.grad(v, y,torch.ones_like(v), create_graph=True)[0]\n",
    "    v_yy = torch.autograd.grad(v_y, y,torch.ones_like(v_y), create_graph=True)[0]\n",
    "    v_t = torch.autograd.grad(v,t,torch.ones_like(v_y), create_graph=True)[0]\n",
    "\n",
    "    p_x = torch.autograd.grad(p,x,torch.ones_like(p), create_graph=True)[0]\n",
    "    p_xx = torch.autograd.grad(p_x,x,torch.ones_like(p_x), create_graph=True)[0]\n",
    "    p_y = torch.autograd.grad(p,y,torch.ones_like(p), create_graph=True)[0]\n",
    "    p_yy = torch.autograd.grad(p_y,y,torch.ones_like(p_y), create_graph=True)[0]\n",
    "\n",
    "\n",
    "    # Compute PDE losses\n",
    "    loss1 = u_t + u * u_x + v * u_y + (1/rho) * p_x - v * (u_xx + u_yy)\n",
    "    loss2 = v_t + u * v_x + v * v_y + (1/rho) * p_y - v * (v_xx + v_yy)\n",
    "    loss3 = (u_x)**2 + 2 * u_y * v_x + (v_y)**2 + (1/rho) * (p_xx + p_yy)\n",
    "\n",
    "    # Store losses in a dictionary\n",
    "    losses = {\n",
    "        'PDE1': loss1,\n",
    "        'PDE2': loss2,\n",
    "        'PDE3': loss3\n",
    "    }\n",
    "    loss_added = loss1+loss2+loss3\n",
    "\n",
    "\n",
    "    return losses, loss_added\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting the boundary conditions \n",
    "We are going to set the boundary conditions for neuman and dirchlit as the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building the NN\n",
    "\n",
    "We are going to build NN with 3 inputs and 3 outpus as we mentioned before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5849)\n",
      "tensor(0.6761)\n",
      "tensor(0.3930)\n",
      "tensor(0.3668)\n",
      "tensor(0.3419)\n",
      "tensor(0.2986)\n",
      "tensor(0.2852)\n",
      "tensor(0.2759)\n",
      "tensor(0.2733)\n",
      "tensor(0.2664)\n",
      "tensor(0.2619)\n",
      "tensor(0.2593)\n",
      "tensor(0.2577)\n",
      "tensor(0.2557)\n",
      "tensor(0.2536)\n",
      "tensor(0.2521)\n",
      "tensor(0.2504)\n",
      "tensor(0.2486)\n",
      "tensor(0.2470)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m top_loss \u001b[38;5;241m+\u001b[39m down_loss \u001b[38;5;241m+\u001b[39m right_loss \u001b[38;5;241m+\u001b[39m left_loss \u001b[38;5;241m+\u001b[39m pde_loss0\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# back propagation\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     56\u001b[0m total_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach())\n",
      "File \u001b[1;32md:\\anaconda\\envs\\MX_project\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda\\envs\\MX_project\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pinn = FCN(3,3,64,4)\n",
    "mse_cost_function = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(pinn.parameters(), lr=1e-3)\n",
    "total_losses = []\n",
    "iteration = []\n",
    "boundary_loss = []\n",
    "\n",
    "\n",
    "for i in range(5001):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #Boundary loss, 1- top \"y=1\"\n",
    "\n",
    "    u, v , p = pinn(x_physics,y_up,\n",
    "                        t_physics)\n",
    "    u_top_loss = mse_cost_function(u, torch.ones((10**3,1)))\n",
    "    v_top_loss = mse_cost_function(v, torch.zeros((10**3,1)))\n",
    "    p_top_loss = mse_cost_function(p, torch.zeros((10**3,1)))\n",
    "    top_loss = u_top_loss + v_top_loss + p_top_loss\n",
    "    #Boundary loss, 2 - down \"y=0\"\n",
    "    u, v , p = pinn(x_physics,y_down.reshape(-1,1),\n",
    "                        t_physics)\n",
    "    u_down_loss = mse_cost_function(u, torch.zeros((10**3,1)))\n",
    "    v_down_loss = mse_cost_function(v, torch.zeros((10**3,1)))\n",
    "    p_y = torch.autograd.grad(p,y_down, torch.ones_like(p), create_graph=True)[0]\n",
    "    p_down_loss = mse_cost_function(p_y, torch.zeros((10**3,1)))   \n",
    "    down_loss = u_down_loss + v_down_loss + p_down_loss  \n",
    "\n",
    "    # Boundary loss, 3 - left \"x=0\" \n",
    "    u, v , p = pinn(x_left,y_physics,\n",
    "                        t_physics)\n",
    "    u_left_loss = mse_cost_function(u, torch.zeros((10**3,1)))\n",
    "    v_left_loss = mse_cost_function(v, torch.zeros((10**3,1)))\n",
    "    p_x = torch.autograd.grad(p,x_left, torch.ones_like(p), create_graph=True)[0]\n",
    "    p_left_loss = mse_cost_function(p_x, torch.zeros((10**3,1))) \n",
    "    left_loss = u_left_loss + v_left_loss + p_left_loss\n",
    "\n",
    "    # Boundary loss, 4 - right \"x=1\" \n",
    "    u, v , p = pinn(x_right.reshape(-1,1),y_physics,\n",
    "                        t_physics)\n",
    "    u_right_loss = mse_cost_function(u, torch.zeros((10**3,1)))\n",
    "    v_right_loss = mse_cost_function(v, torch.zeros((10**3,1)))\n",
    "    p_x = torch.autograd.grad(p,x_right, torch.ones_like(p), create_graph=True)[0]\n",
    "    p_right_loss = mse_cost_function(p_x, torch.zeros((10**3,1))) \n",
    "    right_loss = u_right_loss + v_right_loss + p_right_loss\n",
    "\n",
    "    # pde loss\n",
    "\n",
    "    pde_losses, losses_added = pde_loss(x = x_physics, y = y_physics, t = t_physics, net = pinn  )\n",
    "    pde_loss0 = mse_cost_function(losses_added, torch.zeros((10**3,1)))\n",
    "    loss = top_loss + down_loss + right_loss + left_loss + pde_loss0\n",
    "\n",
    "    # back propagation\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_losses.append(loss.detach())\n",
    "    if i % 5 == 0:\n",
    "        total_losses.append(loss.detach())\n",
    "        print(total_losses[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MX_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
